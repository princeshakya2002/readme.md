{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae60e54-50a3-460a-aadb-d58487af7b7b",
   "metadata": {},
   "source": [
    "Project 7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a91a84d4-3c1e-4e7d-aedc-fbfbcf0f9f60",
   "metadata": {},
   "source": [
    "\n",
    "Propensify (Propensity Model to identify how likely certain target groups customers respond to the marketing campaign)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09a669df-860d-4a7e-95ec-680d99ea1ed7",
   "metadata": {},
   "source": [
    "1. Data Collection\n",
    "Objective: Collect the historical data set and potential customers list provided by the insurance company.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Download and extract the dataset from the provided zip file.\n",
    "Load the data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0179b-78d1-4ee5-81af-835379ad7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load historical data and potential customers list\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40452311-4b22-4d88-8564-50ad439114fc",
   "metadata": {},
   "source": [
    "2. Exploratory Data Analysis (EDA)\n",
    "Objective: Analyze and understand the data to identify patterns, relationships, and trends.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Check data quality, handle missing values, and outliers.\n",
    "Visualize distributions and relationships among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52555300-906c-4959-b9f9-4e103226c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Visualize distributions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='target', data=train_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e46d9286-9235-4ea1-8053-061c277d5686",
   "metadata": {},
   "source": [
    "3. Data Cleaning\n",
    "Objective: Standardize, handle missing values, and address outliers.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Standardize column names and data formats.\n",
    "Impute or remove missing values.\n",
    "Identify and treat outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bb8d6-07a9-454f-948e-b3b7db0781f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "train_data.fillna(train_data.median(), inplace=True)\n",
    "\n",
    "# Treat outliers\n",
    "Q1 = train_data.quantile(0.25)\n",
    "Q3 = train_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "train_data = train_data[~((train_data < (Q1 - 1.5 * IQR)) | (train_data > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63175e21-9482-432a-b5e9-09e5c0e00aed",
   "metadata": {},
   "source": [
    "4. Dealing with Imbalanced Data\n",
    "Objective: Balance the data using appropriate methods.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Use techniques such as SMOTE (Synthetic Minority Over-sampling Technique) to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d089de-f912-48f5-9ff1-eca530df6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    " from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop('target', axis=1)\n",
    "y = train_data['target']\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d6fc19d-74ea-4f55-b8ae-6b3231af90f6",
   "metadata": {},
   "source": [
    "5. Feature Engineering and Selection\n",
    "Objective: Create new features or transform existing features for better model performance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Transform features using techniques such as scaling and encoding.\n",
    "Create interaction terms if necessary.\n",
    "Select relevant features based on domain knowledge or feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02443ac6-f391-43e5-b3f2-fc50beaa9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "678f0c28-664e-49ad-bd6c-a003a2104e0a",
   "metadata": {},
   "source": [
    "6. Train/Test Split\n",
    "Objective: Apply a sampling distribution to find the best split.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb383f-e533-4de6-9e74-5c9c61f462ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df7721b6-7b53-4bca-b2f6-92401ef45aeb",
   "metadata": {},
   "source": [
    "7. Model Selection, Training, Predicting, and Assessment\n",
    "Objective: Choose the most appropriate model, train it, make predictions, and assess its performance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Try multiple classification models such as Logistic Regression, Random Forest, and Gradient Boosting.\n",
    "Train the models on the training data.\n",
    "Make predictions on the test data.\n",
    "Choose the best-performing model based on evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11d97e-ab38-42f7-8082-d664a2797900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Assess model performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8709a8a-27f5-40c0-9754-9e26643c2487",
   "metadata": {},
   "source": [
    "8. Hyperparameter Tuning/Model Improvement\n",
    "Objective: Tune hyperparameters to improve model performance.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Use techniques such as GridSearchCV or RandomizedSearchCV for hyperparameter tuning.\n",
    "Iterate over different parameter combinations to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4ec88-c65e-403c-9e97-c47021c544e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize and train GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and retrain the model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Assess model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b9a230a-991c-436a-91fc-096b2f309c9d",
   "metadata": {},
   "source": [
    "9. Model Deployment Plan\n",
    "Objective: Plan the deployment of the trained model into a production environment.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Decide on the deployment environment (e.g., cloud platform, on-premise server).\n",
    "Develop APIs or scripts to integrate the model into existing systems.\n",
    "Test the deployment thoroughly to ensure functionality and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e091d-9486-4b0d-98f3-08f8f4f1fa5a",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73a40063-74cf-40d6-92fe-315a75384b98",
   "metadata": {},
   "source": [
    "By following this project plan, Propensify will be able to develop, validate, and deploy a propensity model for identifying potential customers. The model will help optimize marketing efforts and improve the effectiveness of campaigns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a036fce-e1ad-421c-8ec9-b940bdd164c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
